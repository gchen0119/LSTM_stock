{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Daily Minimum Temperature Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a daily minimum temperature forecast project using LSTM. The goal is to input a limited sequence of time-series and obtain the following output time-series. \n",
    "\n",
    "The codes for this project is modified from [Thushan Ganegedara's Datacamp tutorial](https://www.datacamp.com/community/tutorials/lstm-python-stock-market). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background for LSTM\n",
    "The long short-term memory (LSTM) unit is an improved version of gated recurrent unit (GRU), which tries to resolve the [vanishing gradient problem](http://neuralnetworksanddeeplearning.com/chap5.html) and keep the long term \"memory\" activated.\n",
    "\n",
    "![alt text](LSTM_rnn.png \"LSTM rnn\")\n",
    "![alt text](LSTM.png \"LSTM cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Picture summary ([Adrew Ng's lecture](https://www.coursera.org/specializations/deep-learning)):\n",
    "\n",
    "> Four parallel layers of interacting networks.\n",
    "\n",
    "> The weighted sum of the input and previous hidden output gets transformed by the activation functions (sigma (0 to 1) and tanh (-1 to 1)).  The `dot`/`add` signs represent elementwise `multiplication`/`addition`. \n",
    "\n",
    "> The cell state c[t] essentially stores the memory, which comes from multiplying the previous cell state to the `forgetness` (0 to 1). This essentially forgets/keeps the previous cell state if forgetness is near zero/one. \n",
    "\n",
    "> The activation/hidden state (last equation) is composed of current state `filter` (0 to 1) by the current cell state activation (-1 to 1) with previous memory. The hidden state connects to the ouput with softmax layer for prediction.\n",
    "\n",
    "> Notice the [`peephole connection`](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf), in other variation of LSTM, is not shown in the figure. It is done by adding another weighted sum of previous cell state c[t-1] to the forget and update gate, and add c[t] to the output gate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the 10 years daily minimum temperature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "series = pd.read_csv('~/Downloads/daily-minimum-temperatures-in-me.csv', error_bad_lines=False)\n",
    "series.rename(columns={'Daily minimum temperatures in Melbourne, Australia, 1981-1990':'mint'},inplace=True) # rename minimum temp to 'mint'\n",
    "y = pd.to_numeric(series[\"mint\"],downcast='float')\n",
    "y.index = pd.DatetimeIndex(start='1981-01-01',end='1990-12-31',freq='d')\n",
    "freq=365 # sampling freq\n",
    "train, valid = y[:freq*9], y[freq*9:]\n",
    "train.index, valid.index = y.index[:freq*9], y.index[freq*9:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class to generate training data, i.e., batches of sequenced data for the input and output (set the random indexing distance for output time series to within 3 indices, this may be a reasonable guess from mid-latitude weather patterns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DataGeneratorSeq(object):\n",
    "    # prices: total training time-series data\n",
    "    # batch_size: the length of a batch/sequence\n",
    "    # num_unroll: sampled number of batches/sequences\n",
    "    # segments: total number of segments in a series that is divided by the batch_size\n",
    "    \n",
    "    def __init__(self,prices,batch_size,num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length //self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b]+1>=self._prices_length:\n",
    "                #self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,3)] \n",
    "            # draw one random index for the output within 3 indices\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        init_data, init_label = None,None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()    \n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the training data batches look like? Set batchsize to 9 samples and 4 time steps, so it'll sample all the first 4 days in January and the prediction output is the randomly indexed (0-3 days) following days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first index of each batch: [0, 364, 728, 1092, 1456, 1820, 2184, 2548, 2912]\n",
      "total number of segments: 364\n",
      "1981-01-01    20.700001\n",
      "1981-01-02    17.900000\n",
      "1981-01-03    18.799999\n",
      "1981-01-04    14.600000\n",
      "1981-01-05    15.800000\n",
      "Freq: D, Name: mint, dtype: float32\n",
      "\n",
      "\n",
      "Unrolled index 0\n",
      "\tInputs:  [20.7 17.4 17.7 16.1 12.  13.3 10.5 11.2 15.2]\n",
      "\n",
      "\tOutputs: [17.9 15.  17.7 18.  12.  13.3 14.6 11.2 15.2]\n",
      "\n",
      "\n",
      "Unrolled index 1\n",
      "\tInputs:  [17.9 17.  16.3 20.4 12.6 11.5 14.7 12.1 17.3]\n",
      "\n",
      "\tOutputs: [14.6 13.5 15.  19.5 12.6 11.5 14.2 16.2 17.3]\n",
      "\n",
      "\n",
      "Unrolled index 2\n",
      "\tInputs:  [18.8 15.  18.4 18.  16.  10.8 14.6 12.7 19.8]\n",
      "\n",
      "\tOutputs: [14.6 15.2 10.9 17.1 16.4 10.8 13.2 14.2  9.5]\n",
      "\n",
      "\n",
      "Unrolled index 3\n",
      "\tInputs:  [14.6 13.5 15.  19.5 16.4 12.  14.2 16.2 15.8]\n",
      "\n",
      "\tOutputs: [15.8 13.5 15.  19.5 13.3 12.  11.7 14.3 15.8]\n"
     ]
    }
   ],
   "source": [
    "tstep = 4\n",
    "batchsize = 9 # a batch contains the samples, not the dimensionality, so each input sample is fed forward once at a time to get an output\n",
    "dg = DataGeneratorSeq(train,batchsize,tstep)\n",
    "print('the first index of each batch: %s'%str(dg._cursor))\n",
    "print('total number of segments: %d'%dg._segments)\n",
    "print(dg._prices.head(5))\n",
    "\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ',dat )\n",
    "    print('\\n\\tOutputs:',lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Unrolled index` is the timesteps, so 20.7, 17.9, 18.8, 14.6 is the four LSTM timesteps of `Inputs` fed forward. The associated four `Outputs` are the randomly indexed (within 0-3 days) four timesteps 20.7, 17.9, 18,8, 15.8. Notice the leading 3 steps of outputs were randomly sampled but identical to the leading 3 steps of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "D = 1 # Dimensionality/Feature of the data. Since the time-series is 1-D this would be 1\n",
    "num_unrollings = 10 # Number of time steps you look into the future.\n",
    "batch_size = 500 # Number of samples in a batch\n",
    "num_nodes = [128,128,128] # Number of hidden nodes in each layer/cell of the deep LSTM stack we're using\n",
    "n_layers = len(num_nodes) # number of layers\n",
    "dropout = 0.2 # dropout amount\n",
    "\n",
    "tf.reset_default_graph() # This is important in case you run this multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining \"tensorized\" training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs, train_outputs = [],[]\n",
    "\n",
    "# You unroll the input over time defining placeholders for each time step\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name='train_inputs_%d'%ui))\n",
    "    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = 'train_outputs_%d'%ui))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining LSTM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTM cells with Xavier initializer. \n",
    "# Which sets small variance for the weights to avoid vanishing/exploding gradient problem \n",
    "# when using tanh as activation function\n",
    "lstm_cells = [ tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n",
    "                            state_is_tuple=True,\n",
    "                            initializer= tf.contrib.layers.xavier_initializer()\n",
    "                           )\n",
    "               for li in range(n_layers) ]\n",
    "\n",
    "# dropout regularization is to reduce overfit (instead of waiting for backprop to find the near zero \n",
    "# weights for regularizatoin, dropout regularization draws a uniformly dist sample between 0 and 1 and \n",
    "# eliminate the nodes with probablity smaller than some keep_prob)\n",
    "drop_lstm_cells = [ tf.contrib.rnn.DropoutWrapper(\n",
    "                   lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout) \n",
    "                   for lstm in lstm_cells ]\n",
    "\n",
    "# create the sequential RNN Cells with dropout regularization\n",
    "multi_cell_drop = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
    "\n",
    "# create the sequential RNN Cells without dropout regularization\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "# The output regression weights that transforms the final hidden layers of LSTM\n",
    "w = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform LSTM hidden to output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor size [nt, batch_size, D]: (10, 500, 1)\n",
      "LSTM hidden tensor size [nt batch_size, hidden nodes]: (10, 500, 128)\n",
      "LSTM hidden tensor size reshaped [batch_size*nt, hidden nodes]: (5000, 128)\n",
      "Final output (w*LSTM_hidden_nodes + b) tensor size [batch_size*nt, hidden nodes]*[hidden nodes,1]=[batch_size*nt,1]: (5000, 1)\n",
      "Split final output into \"nt\" of [batch_size,1] tensors\n"
     ]
    }
   ],
   "source": [
    "# Create cell state 'c' and hidden state 'h' variables to maintain the state of the LSTM\n",
    "c, h = [],[]\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "    c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "    h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "    initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
    "\n",
    "# Do several tensor transformations, because the function dynamic_rnn requires the output to be of\n",
    "# a specific format. Read more at: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "# make input into a tensor of size [ntsteps, batch_size, D]\n",
    "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0) \n",
    "print('Input tensor size [nt, batch_size, D]: '+str(all_inputs.shape))\n",
    "\n",
    "# all_outputs is [seq_length, batch_size, num_nodes]\n",
    "all_lstm_outputs, state = tf.nn.dynamic_rnn(\n",
    "    multi_cell_drop, all_inputs, initial_state=tuple(initial_state),\n",
    "    time_major = True, dtype=tf.float32)\n",
    "print('LSTM hidden tensor size [nt batch_size, hidden nodes]: '+str(all_lstm_outputs.shape))\n",
    "\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n",
    "print('LSTM hidden tensor size reshaped [batch_size*nt, hidden nodes]: '+str(all_lstm_outputs.shape))\n",
    "\n",
    "all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
    "print('Final output (w*LSTM_hidden_nodes + b) tensor size ' \n",
    "      '[batch_size*nt, hidden nodes]*[hidden nodes,1]=[batch_size*nt,1]: '+str(all_outputs.shape))\n",
    "\n",
    "split_outputs = tf.split(all_outputs,num_unrollings,axis=0)\n",
    "print('Split final output into \"nt\" of [batch_size,1] tensors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Calculation and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-f0b050d07929>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f0b050d07929>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    for ui in range(num_unrollings):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# When calculating the loss you need to be careful about the exact form, because you calculate\n",
    "# loss of all the unrolled steps at the same time\n",
    "# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n",
    "                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
    "for ui in range(num_unrollings):\n",
    "    loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
    "    tf_min_learning_rate)\n",
    "\n",
    "# Optimizer.\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "* [Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Why use LSTM? (paper collection)](http://people.idsia.ch/~juergen/rnn.html)\n",
    "* [LSTM for stock prediction, referenece project](https://www.datacamp.com/community/tutorials/lstm-python-stock-market)\n",
    "* [vanishing gradient problem explained](http://neuralnetworksanddeeplearning.com/chap5.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
